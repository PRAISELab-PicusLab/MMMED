{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "P_PRUDH65YgQ",
        "8TJOa-DlIf0v"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfB8zxzMKRSD",
        "outputId": "74105651-8b4f-41b7-c325-cecb677c38e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "\n",
        "# !pip install -U transformers==4.48.3\n",
        "!pip install -U transformers  # Per Qwen2.5-VL\n",
        "!pip install -U accelerate\n",
        "!pip install -U bitsandbytes\n",
        "\n",
        "!pip install triton\n",
        "!pip install tiktoken\n",
        "\n",
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "ike1iQpLAuPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install qwen-vl-utils==0.0.4\n",
        "# !pip install qwen-vl-utils==0.0.8  # Per Qwen2.5-VL\n",
        "\n",
        "# !pip install git+https://github.com/deepseek-ai/DeepSeek-VL.git  # Per DeepSeek-VL\n",
        "!pip install git+https://github.com/shobhitag11/DeepSeek-VL2-Run-On-Google-Colab.git  # Per DeepSeek-VL2"
      ],
      "metadata": {
        "id": "ikmtiA2xvP9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPwFXSuMJgIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76502d69-df96-4b5e-efa4-d80dcbb16014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version is above 3.10, patching the collections module.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import requests\n",
        "import argparse\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers.image_utils import load_image\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    # PaliGemmaProcessor,\n",
        "    # PaliGemmaForConditionalGeneration,\n",
        "    # BitsAndBytesConfig,\n",
        "    # Qwen2VLForConditionalGeneration,\n",
        "    # Qwen2_5_VLForConditionalGeneration,  # Per Qwen2.5-VL\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer, AutoProcessor\n",
        ")\n",
        "\n",
        "# from qwen_vl_utils import process_vision_info\n",
        "\n",
        "from deepseek_vl2.models import DeepseekVLV2Processor\n",
        "from deepseek_vl2.utils.io import load_pil_images\n",
        "\n",
        "# from deepseek_vl.models import VLChatProcessor\n",
        "# from deepseek_vl.utils.io import load_pil_images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLaVa-1.5-7B-HF - Qwen2.5-VL-7B-Instruct - Phi-3.5-vision-instruct"
      ],
      "metadata": {
        "id": "BxphWFn05n0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    return str(text).strip().lower().replace('\\n', ' ').replace(\";\", \"\").replace('\"', \"\")\n",
        "\n",
        "def model_generation(model_name, content, image_url):\n",
        "\n",
        "    if model_name == \"llava-hf/llava-1.5-7b-hf\" or model_name == \"Qwen/Qwen2-VL-7B-Instruct\" \\\n",
        "        or model_name == \"Qwen/Qwen2.5-VL-7B-Instruct\":\n",
        "        messages = [\n",
        "            {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": [\n",
        "                  {\"type\": \"image\", \"url\": f\"{image_url}\"},\n",
        "                  {\"type\": \"text\", \"text\": content},\n",
        "                ],\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        # Generate text based on image and prompt\n",
        "        outputs = image_text_pipeline(text=messages, max_new_tokens=100)\n",
        "        generated_text = outputs[0][\"generated_text\"][-1]['content']\n",
        "        match = re.search(r'{\\s*\"answer\"\\s*:\\s*\"[A-Za-z]\"\\s*}', generated_text, re.DOTALL)\n",
        "        if match:\n",
        "            generated_text = match.group(0)\n",
        "        else:\n",
        "            generated_text = '{\"answer\": \"' + \"{}\".format(generated_text[0]) + '\"}'\n",
        "\n",
        "    elif model_name == \"deepseek-ai/deepseek-vl-7b-chat\" or model_name == \"deepseek-ai/deepseek-vl2-tiny\":\n",
        "        raw_image = requests.get(image_url, stream=True).content\n",
        "        with open('image.png', 'wb') as handler:\n",
        "            handler.write(raw_image)\n",
        "\n",
        "        if model_name.endswith(\"deepseek-vl2-tiny\"):\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"<|User|>\",\n",
        "                    \"content\": f\"<image>\\n<|ref|>{content}<|/ref|>\",\n",
        "                    \"images\": [\"./image.png\"]\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"<|Assistant|>\",\n",
        "                    \"content\": \"\"\n",
        "                }\n",
        "            ]\n",
        "            vlm = model\n",
        "        else:\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"User\",\n",
        "                    \"content\": f\"<image_placeholder>\\n{content}\",\n",
        "                    \"images\": [\"./image.png\"]\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"Assistant\",\n",
        "                    \"content\": \"\"\n",
        "                }\n",
        "            ]\n",
        "            vlm = model.language_model\n",
        "\n",
        "\n",
        "        # load images and prepare for inputs\n",
        "        pil_images = load_pil_images(messages)\n",
        "        prepare_inputs = processor(\n",
        "            conversations=messages,\n",
        "            images=pil_images,\n",
        "            force_batchify=True\n",
        "        ).to(model.device, dtype=torch.float16)\n",
        "\n",
        "        # run image encoder to get the image embeddings\n",
        "        inputs_embeds = model.prepare_inputs_embeds(**prepare_inputs)\n",
        "\n",
        "        # run the model to get the response\n",
        "        outputs = vlm.generate(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=prepare_inputs.attention_mask,\n",
        "            pad_token_id=processor.tokenizer.eos_token_id,\n",
        "            bos_token_id=processor.tokenizer.bos_token_id,\n",
        "            eos_token_id=processor.tokenizer.eos_token_id,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=False,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "        generated_text = processor.tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "        print(generated_text)\n",
        "        match = re.search(r'{\\s*\"answer\"\\s*:\\s*\"[A-Za-z]\"\\s*}', generated_text, re.DOTALL)\n",
        "        if match:\n",
        "            generated_text = match.group(0)\n",
        "        else:\n",
        "            generated_text = '{\"answer\": \"' + \"{}\".format(generated_text[0]) + '\"}'\n",
        "\n",
        "    elif model_name == \"microsoft/Phi-3.5-vision-instruct\":\n",
        "        raw_image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": f\"<|image_1|>\\n {content}\\n ASSISTANT:\"},\n",
        "        ]\n",
        "\n",
        "        prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = processor(prompt, [raw_image], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        generation_args = {\"max_new_tokens\": 100, \"do_sample\": False}\n",
        "        generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)\n",
        "\n",
        "        # remove input tokens\n",
        "        generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
        "        generated_text = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "        match = re.search(r'{\\s*\"answer\"\\s*:\\s*\"[A-Za-z]\"\\s*}', generated_text, re.DOTALL)\n",
        "        if match:\n",
        "            generated_text = match.group(0)\n",
        "        else:\n",
        "            generated_text = '{\"answer\": \"' + \"{}\".format(generated_text) + '\"}'\n",
        "\n",
        "    print(generated_text)\n",
        "    return generated_text\n",
        "\n",
        "def process_csv(df, csv_path, model_name, json_filename, device):\n",
        "    # Check if all required columns exist\n",
        "    required_columns = ['Category', 'Question', 'Image url', 'AnswerA', 'AnswerB',\n",
        "                        'AnswerC', 'AnswerD', 'AnswerE', 'Correct Answer']\n",
        "\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        print(f\"Error: One or more required columns are missing in csv '{csv_path}'.\")\n",
        "        return\n",
        "\n",
        "    # Ensure the JSON file exists\n",
        "    if not os.path.exists(json_filename):\n",
        "        with open(json_filename, 'w') as f:\n",
        "            json.dump([], f)  # Initialize with an empty list\n",
        "\n",
        "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        category = row['Category']\n",
        "        question = row['Question']\n",
        "        image_url = row['Image url']\n",
        "        answers = {k: clean_text(row[k]) for k in ['AnswerA', 'AnswerB', 'AnswerC', 'AnswerD', 'AnswerE']}\n",
        "        answer2key = {v: k[-1] for k, v in answers.items()}  # Extract letter from key (e.g., 'A', 'B', ...)\n",
        "        correct_answer_text = clean_text(row['Correct Answer'])\n",
        "\n",
        "        try:\n",
        "            correct_answer_key = answer2key[correct_answer_text]\n",
        "        except:\n",
        "            print(\"ERROR!\")\n",
        "            print(\"   Question: \", question)\n",
        "            print(\"   Answers: \", answers)\n",
        "            print(\"   Correct answer: \", correct_answer_text)\n",
        "            continue\n",
        "\n",
        "        # If there is an image URL, process with the image-to-text model\n",
        "        if image_url != \"\":\n",
        "            try:\n",
        "                # image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "                content = f\"\"\"You are a medical student who must answer a multiple-choice test.\\n\n",
        "                Given a medical image and a question related to {category}, choose the correct answer from the options.\\n\n",
        "                Question: {question}\\n\n",
        "                A: {answers['AnswerA']}\\n\n",
        "                B: {answers['AnswerB']}\\n\n",
        "                C: {answers['AnswerC']}\\n\n",
        "                D: {answers['AnswerD']}\\n\n",
        "                E: {answers['AnswerE']}\\n\n",
        "                You MUST return an answer EXACTLY in JSON format: {{\"answer\": \"letter\"}}.\\n\n",
        "                In ANY CASE, assign a letter equal to the most appropriate option among those provided.\\n\n",
        "                Do not make arguments or reasoning in your response.\n",
        "                \"\"\"\n",
        "\n",
        "                generated_text = model_generation(model_name, content, image_url)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image for question '{question}': {e}\")\n",
        "                continue\n",
        "\n",
        "        try:\n",
        "            # Convert the generated text to a dictionary\n",
        "            response_dict = json.loads(generated_text)\n",
        "            model_answer = response_dict.get('answer', '').upper()\n",
        "            is_correct = model_answer == correct_answer_key.upper()\n",
        "\n",
        "            result = {\n",
        "                'Category': category,\n",
        "                'Task': 'Multimodal' if image_url else 'Text',\n",
        "                'Model': model_name,\n",
        "                'Question': question,\n",
        "                'Image URL': image_url,\n",
        "                'Answer A': answers['AnswerA'],\n",
        "                'Answer B': answers['AnswerB'],\n",
        "                'Answer C': answers['AnswerC'],\n",
        "                'Answer D': answers['AnswerD'],\n",
        "                'Answer E': answers['AnswerE'],\n",
        "                'Correct Answer': correct_answer_key.upper(),\n",
        "                'Model Answer': model_answer,\n",
        "                'Is Correct': is_correct,\n",
        "            }\n",
        "\n",
        "            # Append result to JSON file\n",
        "            with open(json_filename, 'r+', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                data.append(result)\n",
        "                f.seek(0)\n",
        "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing model's output: {e}\")\n",
        "\n",
        "    print(f'Updated results for sheet \"{category}\" in {json_filename}')\n",
        "\n",
        "def initialize_model(model_name, device):\n",
        "    global image_text_pipeline, model, processor\n",
        "    image_text_pipeline, model, processor = None, None, None\n",
        "    print(\"Initializing the model...\")\n",
        "\n",
        "    # quantization_config = BitsAndBytesConfig(\n",
        "    #     load_in_4bit=True,\n",
        "    #     bnb_4bit_compute_dtype=torch.float16\n",
        "    # )\n",
        "\n",
        "    # Determine the model type based on the task\n",
        "    if model_name == \"llava-hf/llava-1.5-7b-hf\" or model_name == \"Qwen/Qwen2-VL-7B-Instruct\" \\\n",
        "        or model_name == \"Qwen/Qwen2.5-VL-7B-Instruct\":\n",
        "        image_text_pipeline = pipeline(\"image-text-to-text\", model=model_name,\n",
        "                                       model_kwargs={\"quantization_config\": quantization_config})\n",
        "\n",
        "    elif model_name == \"deepseek-ai/deepseek-vl-7b-chat\" or model_name == \"deepseek-ai/deepseek-vl2-tiny\":\n",
        "        if model_name.endswith(\"deepseek-vl2-tiny\"):\n",
        "            processor = DeepseekVLV2Processor.from_pretrained(model_name)\n",
        "        else:\n",
        "            processor = VLChatProcessor.from_pretrained(model_name)\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            # quantization_config=quantization_config,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16\n",
        "        ).cuda().eval()\n",
        "\n",
        "    elif model_name == \"microsoft/Phi-3.5-vision-instruct\":\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            trust_remote_code=True,\n",
        "            _attn_implementation='eager'\n",
        "        )\n",
        "        processor = AutoProcessor.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "    print(\"Model initialized.\")\n",
        "    return image_text_pipeline, model, processor\n",
        "\n",
        "def main(csv_path, colab_path, model_name):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    image_text_pipeline, model, processor = initialize_model(model_name, device)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    model_spec = model_name.split(\"/\")[-1]\n",
        "\n",
        "    json_filename = f\"{colab_path}TASK - MULTIMODAL/LLM/{csv_path.split('/')[-1].split('.')[0]}_{model_spec}_MC.json\"\n",
        "    if os.path.exists(json_filename):\n",
        "        print(f\"Skipping csv '{csv_path.split('/')[-1]}' as output file '{json_filename}' already exists.\")\n",
        "    else:\n",
        "        print(f\"Processing csv '{csv_path.split('/')[-1]}'...\")\n",
        "        process_csv(df, csv_path.split('/')[-1], model_name, json_filename, device)"
      ],
      "metadata": {
        "id": "PHdpo3d7_Teh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colab_path = \"/content/drive/MyDrive/Benchmarking LLM/\"\n",
        "csv_path = f\"{colab_path}Datasets/Multimodal/MIR_Multimodal_Dataset_English_Prepro.csv\"\n",
        "model_name = \"deepseek-ai/deepseek-vl2-tiny\"\n",
        "\n",
        "main(csv_path, colab_path, model_name)"
      ],
      "metadata": {
        "id": "pF91NtB3bjum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paligemma 2 - 3B Mix 448x448"
      ],
      "metadata": {
        "id": "P_PRUDH65YgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    return str(text).strip().lower().replace('\\n', ' ').replace(\";\", \"\").replace('\"', \"\")\n",
        "\n",
        "def process_csv(df, csv_name, model_name, json_filename, device):\n",
        "    required_columns = ['Category', 'Question', 'Image url', 'AnswerA', 'AnswerB',\n",
        "                        'AnswerC', 'AnswerD', 'AnswerE', 'Correct Answer']\n",
        "\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        print(f\"Error: One or more required columns are missing in sheet '{csv_name}'.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(json_filename):\n",
        "        with open(json_filename, 'w') as f:\n",
        "            json.dump([], f)\n",
        "\n",
        "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        category = row['Category']\n",
        "        original_question = row['Question']\n",
        "        image_url = row['Image url']\n",
        "        answers = {k: clean_text(row[k]) for k in ['AnswerA', 'AnswerB', 'AnswerC', 'AnswerD', 'AnswerE']}\n",
        "        answer2key = {v: k[-1] for k, v in answers.items()}  # Extract letter from key (e.g., 'A', 'B', ...)\n",
        "        correct_answer_text = clean_text(row['Correct Answer'])\n",
        "\n",
        "        try:\n",
        "            correct_answer_key = answer2key[correct_answer_text]\n",
        "        except:\n",
        "            print(\"ERROR!\")\n",
        "            print(\"   Question: \", original_question)\n",
        "            print(\"   Answers: \", answers)\n",
        "            print(\"   Correct answer: \", correct_answer_text)\n",
        "            continue\n",
        "\n",
        "        if image_url:\n",
        "            try:\n",
        "                raw_image = load_image(image_url)\n",
        "\n",
        "                ################ PROMPT DA CONTROLLARE ED AGGIUSTARE ################\n",
        "                ################ LA LINGUA IN BASE AL DATASET #######################\n",
        "                question = f\"\"\"You are a medical student who must answer a multiple-choice test.\\n\n",
        "                Given a medical image and a question related to {category}, choose the correct answer from the options.\\n\n",
        "                Question: {original_question}\\n\n",
        "                A: {answers['AnswerA']}\\n\n",
        "                B: {answers['AnswerB']}\\n\n",
        "                C: {answers['AnswerC']}\\n\n",
        "                D: {answers['AnswerD']}\\n\n",
        "                E: {answers['AnswerE']}\\n\n",
        "                You MUST return an answer in JSON format: {{\"answer\": \"letter\"}}.\\n\n",
        "                In ANY CASE, assign a letter equal to the most appropriate option among those provided.\n",
        "                \"\"\"\n",
        "\n",
        "                lang = \"en\"\n",
        "                prompt = f\"<image> answer {lang} {question}\"\n",
        "                ################ FINE PROMPT ########################################\n",
        "\n",
        "                model_inputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\").to(torch.float16).to(device)\n",
        "                input_len = model_inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "                generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
        "                generation = generation[0][input_len:]\n",
        "                generated_text = processor.decode(generation, skip_special_tokens=True)\n",
        "                # print(generated_text)\n",
        "\n",
        "                if generated_text:\n",
        "                    generated_text = '{\"answer\": \"' + \"{}\".format(generated_text[0]) + '\"}'\n",
        "                else:\n",
        "                    generated_text = '{\"answer\": \"\"}'\n",
        "                print(generated_text)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image for question '{original_question}': {e}\")\n",
        "                continue\n",
        "\n",
        "        try:\n",
        "            response_dict = json.loads(generated_text)\n",
        "            model_answer = response_dict.get('answer', '').upper()\n",
        "            is_correct = model_answer == correct_answer_key.upper()\n",
        "\n",
        "            result = {\n",
        "                'Category': category,\n",
        "                'Task': 'Multimodal' if image_url else 'Text',\n",
        "                'Model': model_name,\n",
        "                'Question': original_question,\n",
        "                'Image URL': image_url,\n",
        "                'Answer A': answers['AnswerA'],\n",
        "                'Answer B': answers['AnswerB'],\n",
        "                'Answer C': answers['AnswerC'],\n",
        "                'Answer D': answers['AnswerD'],\n",
        "                'Answer E': answers['AnswerE'],\n",
        "                'Correct Answer': correct_answer_key.upper(),\n",
        "                'Model Answer': model_answer,\n",
        "                'Is Correct': is_correct,\n",
        "            }\n",
        "\n",
        "            with open(json_filename, 'r+', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                data.append(result)\n",
        "                f.seek(0)\n",
        "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing model's output: {e}\")\n",
        "\n",
        "    print(f'Updated results for sheet \"{category}\" in {json_filename}')\n",
        "\n",
        "def initialize_model(model_name, device):\n",
        "    global model, processor\n",
        "    print(\"Initializing the PaliGemma model...\")\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    ).to(device)\n",
        "    processor = PaliGemmaProcessor.from_pretrained(model_name)\n",
        "\n",
        "    print(\"Model initialized.\")\n",
        "    return model, processor\n",
        "\n",
        "def main(csv_path, colab_path, model_name):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model, processor = initialize_model(model_name, device)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    model_spec = model_name.split(\"/\")[-1]\n",
        "\n",
        "    json_filename = f\"{colab_path}TASK - MULTIMODAL/LLM/{csv_path.split('/')[-1].split('.')[0]}_{model_spec}_MC.json\"\n",
        "    if os.path.exists(json_filename):\n",
        "        print(f\"Skipping csv '{csv_path.split('/')[-1]}' as output file '{json_filename}' already exists.\")\n",
        "    else:\n",
        "        print(f\"Processing csv '{csv_path.split('/')[-1]}'...\")\n",
        "        process_csv(df, csv_path.split('/')[-1], model_name, json_filename, device)\n"
      ],
      "metadata": {
        "id": "gsMRL2gE5XXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colab_path = \"/content/drive/MyDrive/Benchmarking LLM/\"\n",
        "csv_path = f\"{colab_path}Datasets/Multimodal/MIR_Multimodal_Dataset_Spanish_Prepro.csv\"\n",
        "model_name = \"google/paligemma2-3b-mix-448\"\n",
        "\n",
        "main(csv_path, colab_path, model_name)"
      ],
      "metadata": {
        "id": "0s7HIkwL4-St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paligemma 2 - 10B Mix 448x448"
      ],
      "metadata": {
        "id": "8TJOa-DlIf0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colab_path = \"/content/drive/MyDrive/Benchmarking LLM/\"\n",
        "csv_path = f\"{colab_path}Datasets/Multimodal/MIR_Multimodal_Dataset_Italian.csv\"\n",
        "model_name = \"google/paligemma2-10b-mix-448\"\n",
        "\n",
        "main(csv_path, colab_path, model_name)"
      ],
      "metadata": {
        "id": "D8PdCdyCK5gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltp0kYR9MlN4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}